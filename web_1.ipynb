{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes all the necessary libraries\n",
    "from selenium_manager import *\n",
    "from data_manger import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'London'\n",
    "job_title = 'data+scientist'\n",
    "num_page = 35  # means its 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web link of page\n",
    "from email import header\n",
    "\n",
    "\n",
    "url = f'https://uk.indeed.com/jobs?q={job_title}&l={location}'\n",
    "pages = f'https://uk.indeed.com/jobs?q=q={job_title}&l={location}&start={num_page}'\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import header\n",
    "from urllib import response\n",
    "\n",
    "\n",
    "def client_handle(url):\n",
    "    # Create chrome web driver objects\n",
    "    driver = webdriver.Chrome()\n",
    "    \"\"\"Get's the url and post \n",
    "    data the webdriver\"\"\"\n",
    "    driver.get(url)\n",
    "    # Clears the cookies pop-up\n",
    "    driver.find_element(\n",
    "        By.XPATH, '//button[text()=\"Accept All Cookies\"]').click()\n",
    "    return driver\n",
    "\n",
    "\n",
    "# Returns job roles\n",
    "def get_roles(driver):\n",
    "    titles = driver.find_elements(By.CLASS_NAME, \"jcs-JobTitle[href]\")\n",
    "    roles = []\n",
    "    for title in titles:\n",
    "        roles.append(title.text)\n",
    "    return roles\n",
    "\n",
    "\n",
    "# Returns Companies names\n",
    "def get_employers(driver):\n",
    "    employers = driver.find_elements(By.CLASS_NAME, 'companyName')\n",
    "    companies = []\n",
    "    for company in employers:\n",
    "        companies.append(company.text)\n",
    "    return companies\n",
    "\n",
    "\n",
    "# Returns job locations\n",
    "def get_location(driver):\n",
    "    job_locations = driver.find_elements(By.CLASS_NAME, 'companyLocation')\n",
    "    locations = []\n",
    "    for local in job_locations:\n",
    "        locations.append(local.text)\n",
    "    return locations\n",
    "\n",
    "\n",
    "# Returns job links\n",
    "def get_links(driver):\n",
    "    web_links = driver.find_elements(By.CLASS_NAME, 'jcs-JobTitle[href]')\n",
    "    links = []\n",
    "    for link in web_links:\n",
    "        links.append(link.get_attribute('href'))\n",
    "    return links\n",
    "\n",
    "\n",
    "# Returns salary info\n",
    "def get_salary(driver):\n",
    "    salary_info = driver.find_elements(\n",
    "        By.CLASS_NAME, 'salary-snippet-container')\n",
    "    salary = []\n",
    "    for sal in salary_info:\n",
    "        salary.append(sal.text)\n",
    "    return salary\n",
    "\n",
    "\n",
    "# Returns Job description\n",
    "def get_description(driver):\n",
    "    descriptions_info = driver.find_elements(By.CLASS_NAME, 'job-snippet')\n",
    "    descriptions = []\n",
    "    for description in descriptions_info:\n",
    "        descriptions.append(description.text)\n",
    "    return descriptions\n",
    "\n",
    "\n",
    "def pages(driver):\n",
    "    # Navigate to the next page\n",
    "    for page in range(0, num_page):\n",
    "        url = url + str(page * 10)\n",
    "        driver.get(url)\n",
    "        sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(num_page):\n",
    "    url = url + str(page * 10)\n",
    "    .get(url)\n",
    "    sleep(2)  # Add a short delay to allow the page to load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    job_listings = soup.find_all(\"div\", class_=\"jobsearch-SerpJobCard\")\n",
    "\n",
    "    for job in job_listings:\n",
    "        job_title = job.find(\"a\", class_=\"jobtitle\").text.strip()\n",
    "        company = job.find(\"span\", class_=\"company\").text.strip()\n",
    "        location = job.find(\"span\", class_=\"location\").text.strip()\n",
    "\n",
    "        print(\"Job Title:\", job_title)\n",
    "        print(\"Company:\", company)\n",
    "        print(\"Location:\", location)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Close the WebDriver after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = client_handle(url)\n",
    "# webpages = pages(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from functions\n",
    "roles = get_roles(driver)\n",
    "companies = get_employers(driver)\n",
    "salary = get_salary(driver)\n",
    "locations = get_location(driver)\n",
    "links = get_links(driver)\n",
    "descriptions = get_description(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple data Series\n",
    "roles_series = pd.Series(roles)\n",
    "companies_series = pd.Series(companies)\n",
    "salary_series = pd.Series(salary)\n",
    "location_series = pd.Series(locations)\n",
    "links_series = pd.Series(links)\n",
    "descriptions_series = pd.Series(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Created dict for data Series\n",
    "job_postings = {'Roles': roles_series,\n",
    "                'Companies': companies_series,\n",
    "                'Salary': salary_series,\n",
    "                'Location': locations,\n",
    "                'Descriptions': descriptions_series,\n",
    "                'Links': links_series}\n",
    "# Creates data frame\n",
    "data = pd.DataFrame(job_postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creates a csv with the collected data\n",
    "data.to_csv('today1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
